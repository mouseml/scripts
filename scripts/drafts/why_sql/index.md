# ЗАЧЕМ НУЖЕН SQL

> Обложка: SQL не нужен?

## Вступление

SQL — три буквы, без которых вас не возьмут на работу. Знание SQL требуют в каждой вакансии и иногда дают задачи на Live Code секциях.

При этом SQL находится в каком-то другом мире. Любую задачу от базовой аналитики до обучения модели можно выполнить без него.

Но на работе никто не даст вам `train.csv`. Вам назовут задачу. Нужно будет понять задачу и наши возможности исходя из доступности данных. Именно для этого в большинстве случаев понадобится SQL.

## Понять задачу

В задачах на курсах или площадках соревнований вся работа уже проделана. Вы можете быть уверены, что это нужная и решаемая задача, ведь кто-то подробно ее обсудил и подготовил данные. Вам остается только пробовать разные модели и подбирать гиперпараметры.

На работе все будет не так. Ваша задача начнется с максимально общей формулировки. Например, вы устроились в Data Science отдел сети супермаркетов. У нас проблемы с планированием закупок. Посмотри что можешь сделать.

Вы не знаете ничего ни о компании, ни о данных, и плохо представляете проблемы планирования. Вы идете в IT отдел, вам дают доступ к базе данных. Вас предупреждают, что база новая, и там все плохо. Большая часть информации разбросана по разным 1C базам у бухгалтерии, маркетинга и региональных менеджеров.

Вам нужно изучить данные и понять задачу. В роли базы данных у нас будет SQLite. В конце видео я подниму Postgres в облаке, посмотрим как те же действия будут выглядеть там.

Если хотите повторять вместе со мной, скачайте файл `data.db` из моей группы в Telegram. В конце обсудим как подключение выглядит на настоящей работе.

Для подключения к базе данных выполните команду:

```shell
sqlite3 data.db
```

Посмотрим какие есть таблицы

```sqlite
.tables
```
```text {.no-copy}
sales   stores
```

Посмотрим на данные в каждой таблице. Отображаем все колонки в таблице `sales` и ограничиваем вывод первыми 5 строками:

```sqlite
SELECT * FROM sales LIMIT 5;
```
```text {.no-copy}
┌─────────┬──────────┬────────────┬───────┬─────────────┬────────────┬──────────┬────────┐
│ sale_id │ store_id │    date    │ month │ day_of_week │ is_holiday │ is_promo │ amount │
├─────────┼──────────┼────────────┼───────┼─────────────┼────────────┼──────────┼────────┤
│ 0       │ 1        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 315780 │
│ 1       │ 2        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 363840 │
│ 2       │ 3        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 498840 │
│ 3       │ 4        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 839700 │
│ 4       │ 5        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 289320 │
└─────────┴──────────┴────────────┴───────┴─────────────┴────────────┴──────────┴────────┘
```

Видим ID строки, магазина и дату. Для удобства из даты выделены месяц и день недели: 1 означает понедельник, а 7 — воскресенье. Если в этот день был государственный праздник, флаг `is_holiday` равен 1. Флаг `is_promo` означает, что в этот день в магазине проводили акцию. Последняя колонка — выручка магазина в этот день в рублях.

Посмотрим на колонки в таблице `stores`:

```sqlite
SELECT * FROM stores LIMIT 5;
```
```text {.no-copy}
┌──────────┬─────────────┬─────────────┬──────────────────────┐
│ store_id │ store_type  │ assortment  │ competition_distance │
├──────────┼─────────────┼─────────────┼──────────────────────┤
│ 1        │ супермаркет │ расширенный │ 1270                 │
│ 2        │ стандарт    │ расширенный │ 570                  │
│ 3        │ стандарт    │ расширенный │ 14130                │
│ 4        │ супермаркет │ базовый     │ 620                  │
│ 5        │ стандарт    │ расширенный │ 29910                │
└──────────┴─────────────┴─────────────┴──────────────────────┘
```

Здесь хранятся ID магазина, тип магазина и ассортимента и расстояние до ближайшего магазина-конкурента в метрах.

Посчитаем количество продаж и количество торговых точек. Для этого используем функцию `COUNT`:

```sqlite
SELECT COUNT(*) FROM sales;
```
```text {.no-copy}
┌──────────┐
│ COUNT(*) │
├──────────┤
│ 808454   │
└──────────┘
```

У нас 808 тысяч записей о продажах. Посчитаем количество магазинов:

```sqlite
SELECT COUNT(*) FROM stores;
```
```text {.no-copy}
┌──────────┐
│ COUNT(*) │
├──────────┤
│ 1112     │
└──────────┘
```

Магазинов 1112. 

Проверим диапазон времени — для этого используем функции `MIN` и `MAX`:

```sqlite
SELECT MIN(date), MAX(date) FROM sales;
```
```text {.no-copy}
┌────────────┬────────────┐
│ MIN(date)  │ MAX(date)  │
├────────────┼────────────┤
│ 2023-01-01 │ 2025-07-31 │
└────────────┴────────────┘
```

Данные доступны с 1 января 2023 года до 31 июля 2025.

Посмотрим минимальный, средний и максимальный объем продаж:

```sqlite
SELECT MIN(amount), AVG(amount), MAX(amount) FROM sales;
```
```text {.no-copy}
┌─────────────┬──────────────────┬─────────────┐
│ MIN(amount) │   AVG(amount)    │ MAX(amount) │
├─────────────┼──────────────────┼─────────────┤
│ 153420      │ 409014.884755348 │ 930480      │
└─────────────┴──────────────────┴─────────────┘
```

Продажи по магазинам составляют от 150 до 930 тысяч, 410 тысяч в среднем.

Мы можем даже провести небольшую аналитику и сравнить продажи в разные дни недели. Для этого выбираем день недели, средние продажи и группируем результат по дням недели:

```sqlite
SELECT day_of_week, AVG(amount) FROM sales GROUP BY day_of_week;
```
```text {.no-copy}
┌─────────────┬──────────────────┐
│ day_of_week │   AVG(amount)    │
├─────────────┼──────────────────┤
│ 1           │ 414162.389284508 │
│ 2           │ 396384.768643111 │
│ 3           │ 398410.75425161  │
│ 4           │ 415123.541302665 │
│ 5           │ 362229.093652136 │
│ 6           │ 468445.066562255 │
│ 7           │ 467223.357873807 │
└─────────────┴──────────────────┘
```

Ожидаемо пик продаж приходится на субботу и воскресенье. Более глубокую аналитику лучше делать в ноутбуках. Для этого нужно сначала забрать нужные данные и сохранить их в CSV файл.

Нам говорят давайте обучать модель

## Получить данные

В соревнованиях у нас есть `train.csv` и `test.csv`. Сейчас у нас есть только база данных.

Посмотрим на таблицу `sales` еще раз:

```sqlite
SELECT * FROM sales LIMIT 5;
```
```text {.no-copy}
┌─────────┬──────────┬────────────┬───────┬─────────────┬────────────┬──────────┬────────┐
│ sale_id │ store_id │    date    │ month │ day_of_week │ is_holiday │ is_promo │ amount │
├─────────┼──────────┼────────────┼───────┼─────────────┼────────────┼──────────┼────────┤
│ 0       │ 1        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 315780 │
│ 1       │ 2        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 363840 │
│ 2       │ 3        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 498840 │
│ 3       │ 4        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 839700 │
│ 4       │ 5        │ 2025-07-31 │ 7     │ 4           │ 0          │ 1        │ 289320 │
└─────────┴──────────┴────────────┴───────┴─────────────┴────────────┴──────────┴────────┘
```

В ней нет данных о магазинах, но мы можем попробовать обучить первую модель без них. Нам понадобятся только колонки с номером месяца, дня недели, информация о праздниках, промо-акциях и объем продаж для предсказания:

```sqlite
SELECT month, day_of_week, is_promo, is_holiday, amount FROM sales LIMIT 5;
```
```text {.no-copy}
┌───────┬─────────────┬──────────┬────────────┬────────┐
│ month │ day_of_week │ is_promo │ is_holiday │ amount │
├───────┼─────────────┼──────────┼────────────┼────────┤
│ 7     │ 4           │ 1        │ 0          │ 315780 │
│ 7     │ 4           │ 1        │ 0          │ 363840 │
│ 7     │ 4           │ 1        │ 0          │ 498840 │
│ 7     │ 4           │ 1        │ 0          │ 839700 │
│ 7     │ 4           │ 1        │ 0          │ 289320 │
└───────┴─────────────┴──────────┴────────────┴────────┘
```

Перейдем к Python проекту. Для работы с данными будем использовать Pandas, а в качестве модели возьмем CatBoost. Установим все зависимости:

```shell
uv add pandas catboost
```

Загрузим данные из базы данных и сохраним их в CSV файл, который потом можно будет прочитать. Для этого используем Pandas. Создаем модуль `load.py`, импортируем встроенную библиотеку `sqlite3` и `pandas`. Переписываем SQL запрос. Заберем все данные до начала 2025 года для обучения:

```python title="load.py"
import sqlite3
import pandas as pd

query = """
SELECT
    month,
    day_of_week,
    is_promo,
    is_holiday,
    amount
FROM sales
WHERE date < '2025-01-01';
"""
```

Подключаемся к базе данных с помощью контекстного менеджера `sqlite3.connect` и передаем ему имя файла с базой данных. Выполняем запрос с помощью функции `read_sql`. Выводим на экран первые пять строк таблицы и ее размерность для проверки и записываем ее в файл `train.csv`:

```python title="load.py" hl_lines="15-20"
import sqlite3
import pandas as pd

query = """
SELECT
    month,
    day_of_week,
    is_promo,
    is_holiday,
    amount
FROM sales
WHERE date < '2025-01-01';
"""

with sqlite3.connect("data.db") as connection:
    data = pd.read_sql(query, connection)

print(data.head())
print(data.shape)
data.to_csv("train.csv", index=False)
```
```text {.no-copy}
   month  day_of_week  is_promo  is_holiday  amount
0     12            2         0           0  156300
1     12            2         0           0  228240
2     12            2         0           0  609120
3     12            2         0           0  156240
4     12            2         0           0  313140
(619945, 5)
```

В выборке обучения будет 620 тысяч строк. Теперь сохраним тестовую выборку — для нее возьмем данные с начала 2025 года. Меняем знак "<" на ">=" и имя файла на `test.csv`:

```python title="load.py" hl_lines="12 18"
import sqlite3
import pandas as pd

query = """
SELECT
    month,
    day_of_week,
    is_promo,
    is_holiday,
    amount
FROM sales
WHERE date >= '2025-01-01';
"""

with sqlite3.connect("data.db") as connection:
    data = pd.read_sql(query, connection)

print(data.head())
print(data.shape)
data.to_csv("test.csv", index=False)
```
```text {.no-copy}
   month  day_of_week  is_promo  is_holiday  amount
0      7            4         1           0  315780
1      7            4         1           0  363840
2      7            4         1           0  498840
3      7            4         1           0  839700
4      7            4         1           0  289320
(188509, 5)
```

В тестовой выборке будет 180 тысяч строк.

Перейдем к обучению. Импортируем `pandas` для работы с данными и читаем таблицы `train.csv` и `test.csv`, которые мы только что сохранили. Убираем из таблицы с признаками колонку с объемом продаж — ее будет предсказывать модель. Повторяем все действия для выборки обучения и тестовой выборки:

```python title="train.py"
import pandas as pd

train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

X_train = train_data.drop(columns=["amount"])
y_train = train_data["amount"]
X_test = test_data.drop(columns=["amount"])
y_test = test_data["amount"]
```

Импортируем модель `CatBoostRegressor` из библиотеки `catboost`. В качестве метрики используем средний процент отклонения из пакета `sklearn.metrics`. Инициализируем модель, вызываем метод `fit` и передаем ему данные выборки обучения. После обучения вычисляем средний процент отклонения модели на тестовой выборке и выводим его значение на экран:

```python title="train.py" hl_lines="2-3 13-17"
import pandas as pd
from catboost import CatBoostRegressor
from sklearn.metrics import mean_absolute_percentage_error

train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

X_train = train_data.drop(columns=["amount"])
y_train = train_data["amount"]
X_test = test_data.drop(columns=["amount"])
y_test = test_data["amount"]

model = CatBoostRegressor()
model.fit(X_train, y_train)

score = mean_absolute_percentage_error(y_test, model.predict(X_test))
print(f"Ошибка на тестовой выборке: {100 * score:.1f}%")
```
```text {.no-copy
Ошибка на тестовой выборке: 27.0%
```
90+75+55+90+100

Получаем среднее отклонение на тестовой выборке в 27%. Например, если реальная выручка магазина за сутки составила 600 тысяч рублей, модель в среднем может предсказать выручку от 400 до 800 тысяч.

## Объединить таблицы

Как на продажи влияют общие факторы: время года, праздники, выходные и распродажи. Но при этом модель ничего не знает о крупных и 

Отклонение в 30% можно объяснить. Мы использовали всего 4 признака. Модель знает только общие сезонные закономерности — как в среднем продажи распределяются по месяцам в году и по дням недели, как на продажи влияют акции и праздники. Но мы не учитываем данные магазинов: тип магазина, ассортимент и насколько близко к нему есть магазины конкурентов.

Для того чтобы собрать такой набор данных, нужно объединить две таблицы в одну. Для этого SQL нужно дать понять, как соединить строки. Для этого нужны ID или ключи. Они уникальные для каждой строки и называются "PRIMARY KEY". В таблице `sales` ключ `sale_id`, а в таблице `stores` — `store_id`.

Кроме ID строки в таблице `sales` есть ID магазина `store_id` — его называют "FOREIGN KEY". Ключи `store_id` в таблице `sales` и `stores` должны совпасть, так мы будем знать что речь идет об одном и том же магазине.

После объединения мы можем обращаться к колонкам в обеих таблицах. Возьмем колонки `date` и `amount` из таблицы `sales` и колонку `store_type` из таблицы `stores`. Объединим их используя ключ `store_id`:

```sqlite
SELECT date, store_type, amount FROM sales JOIN stores USING(store_id) LIMIT 5;
```
```text {.no-copy}
┌────────────┬─────────────┬────────┐
│    date    │ store_type  │ amount │
├────────────┼─────────────┼────────┤
│ 2025-07-31 │ супермаркет │ 315780 │
│ 2025-07-31 │ стандарт    │ 363840 │
│ 2025-07-31 │ стандарт    │ 498840 │
│ 2025-07-31 │ супермаркет │ 839700 │
│ 2025-07-31 │ стандарт    │ 289320 │
└────────────┴─────────────┴────────┘
```

В результате мы получаем колонки из обеих таблиц.

В объединенной таблице можно найти больше интересных закономерностей. Например, какие продажи в среднем у магазинов каждого типа:

```sqlite
SELECT store_type, AVG(amount) FROM sales JOIN stores USING(store_id) GROUP BY store_type;
```
```text {.no-copy}
┌─────────────┬──────────────────┐
│ store_type  │   AVG(amount)    │
├─────────────┼──────────────────┤
│ мини        │ 406426.804514395 │
│ стандарт    │ 407167.354313378 │
│ супермаркет │ 410051.590890435 │
│ универсам   │ 512573.170656371 │
└─────────────┴──────────────────┘
```

Тут выделяются универсамы — их выручка больше в среднем на четверть.

Можно посмотреть выручку в разрезе ассортимента:

```sqlite
SELECT assortment, AVG(amount) FROM sales JOIN stores USING(store_id) GROUP BY assortment;
```
```text {.no-copy}
┌─────────────┬──────────────────┐
│ assortment  │   AVG(amount)    │
├─────────────┼──────────────────┤
│ базовый     │ 424229.478572588 │
│ расширенный │ 394016.517298669 │
│ широкий     │ 493714.423945297 │
└─────────────┴──────────────────┘
```

Тут интересно, что товары первой необходимости из базового ассортимента приносят большую выручку по сравнению с расширенным ассортиментом.

Вернемся к обучению. Обновим файлы `train.csv` и `test.csv`. Добавим к ним колонки из таблицы `stores`: тип магазина, ассортимент и расстояние до ближайшего магазина-конкурента. Чтобы получить доступ к этим колонкам добавляем команду `INNER JOIN` для объединения таблиц по ключу `store_id`:

```python title="load.py"
import sqlite3
import pandas as pd

query = """
SELECT
    month,
    day_of_week,
    is_promo,
    is_holiday,
    amount,
    store_type,
    assortment,
    competition_distance
FROM sales
JOIN stores USING(store_id)
WHERE date < '2025-01-01';
"""

with sqlite3.connect("data.db") as connection:
    data = pd.read_sql(query, connection)

print(data.head())
print(data.shape)
data.to_csv("train.csv", index=False)
```
```text
   month  day_of_week  is_promo  is_holiday  amount   store_type   assortment  competition_distance
0     12            2         0           0  156300  супермаркет  расширенный                  1270
1     12            2         0           0  228240     стандарт  расширенный                 14130
2     12            2         0           0  609120  супермаркет      базовый                   620
3     12            2         0           0  156240     стандарт  расширенный                   310
4     12            2         0           0  313140     стандарт      базовый                 24000
(619945, 8)
```

```python title="load.py"
import sqlite3
import pandas as pd

query = """
SELECT
    month,
    day_of_week,
    is_promo,
    is_holiday,
    amount,
    store_type,
    assortment,
    competition_distance
FROM sales
JOIN stores USING(store_id)
WHERE date >= '2025-01-01';
"""

with sqlite3.connect("data.db") as connection:
    data = pd.read_sql(query, connection)

print(data.head())
print(data.shape)
data.to_csv("test.csv", index=False)
```
```text
   month  day_of_week  is_promo  is_holiday  amount   store_type   assortment  competition_distance
0      7            4         1           0  315780  супермаркет  расширенный                  1270
1      7            4         1           0  363840     стандарт  расширенный                   570
2      7            4         1           0  498840     стандарт  расширенный                 14130
3      7            4         1           0  839700  супермаркет      базовый                   620
4      7            4         1           0  289320     стандарт  расширенный                 29910
(188509, 8)
```

Меняем условие и сохраняем тестовую выборку.

Обучим модель на новых данных. Теперь в них есть два категориальных признака: `store_type` и `assortment`. Передаем их конструктору `CatBoostRegressor` и перезапускаем обучение:

```python title="train.py" hl_lines="13"
import pandas as pd
from catboost import CatBoostRegressor
from sklearn.metrics import mean_absolute_percentage_error

train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

X_train = train_data.drop(columns=["amount"])
y_train = train_data["amount"]
X_test = test_data.drop(columns=["amount"])
y_test = test_data["amount"]

model = CatBoostRegressor(cat_features=["store_type", "assortment"])
model.fit(X_train, y_train)

score = mean_absolute_percentage_error(y_test, model.predict(X_test))
print(f"Ошибка на тестовой выборке: {100 * score:.1f}%")
```
```text {.co-copy}
Ошибка на тестовой выборке: 18.4%
```

Ошибка снизилась на треть — с 27% до 18%. 

## Как оно в реальности

В этом примере у нас на руках оказалась копия базы данных. Мы перекладываем данные из одного файла в другой.

На самом деле будет база данных в облаке. Вам дадут ее адрес, логин и пароль. В зависимости от типа базы вы будете использовать не sqlite3 а другой клиент. Но ничего больше не изменится. Например, если это Postgres, установим `psycopg`, поменяем вызов соединения с базой:

```python title="load.py"
import pandas as pd
from sqlalchemy import create_engine

query = """
SELECT
    month,
    day_of_week,
    is_promo,
    is_holiday,
    amount,
    store_type,
    assortment,
    competition_distance
FROM sales
JOIN stores USING(store_id)
WHERE date < '2025-01-01';
"""

engine = create_engine("postgresql://ivanov:ivanov123@5.129.250.215:5432/supermarket")
data = pd.read_sql(query, engine)

data.to_csv("train.csv", index=False)
```

Я поднял базу в облаке и переложил в нее те же данные. Получаем тот же результат.

+ тут же можно переключиться обратно на sqlite:

```python
engine = create_engine("postgresql:///data.db")
```
